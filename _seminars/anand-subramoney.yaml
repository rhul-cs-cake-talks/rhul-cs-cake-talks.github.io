---
topic: Machine Learning
type: Technical
title: Emergent behaviour in large deep learning models
speaker: Anand Subramoney
institution: RHUL
webpage: https://anandsubramoney.com/
# date: 08/11/2023, 15:00
date: 2023-11-08
venue: Bedford 0-07
link: https://teams.microsoft.com/l/meetup-join/19%3ameeting_YWUwNGJiODYtODg3Zi00OTJhLWE2YmQtZDU2YWU2ODUzNjdh%40thread.v2/0?context=%7b%22Tid%22%3a%222efd699a-1922-4e69-b601-108008d28a2e%22%2c%22Oid%22%3a%22a039aadc-3c03-47d9-9b54-085490a7ce53%22%7d
recording: https://rhul.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=68b98910-abd1-479d-a160-b0b600d70d5c
bio: 
abstract: >
  Recent years have seen remarkable progress in the capabilities of large deep learning models, especially in language processing. Large language models such as GPT-3 derive a lot of their utility from being zero or few-shot learners, which are instances of a phenomenon labelled in-context learning -- the ability to perform a diverse range of tasks simply based on demonstration examples provided as input context, without any gradient updates to the model parameters. This emergent capability is poorly understood and has intrigued the ML research community. In this talk, I will provide an overview of in-context learning and survey the current state of understanding regarding the emergence of these new abilities in large language models. Then, I will examine the extent to which in-context learning needs large-scale models and the contribution of model architectures versus training approaches. Finally, I will analyse and relate this phenomenon to meta-learning approaches with recurrent architectures that predate large language models.

---