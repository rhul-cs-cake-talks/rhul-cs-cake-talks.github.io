---
topic: Machine Learning
type: Technical
title: >
  Simulation-Based Calibration of Confidence Sets for Statistical Models
speaker: Rafael Izbicki
institution: Federal University of São Carlos, Brazil
webpage:  
date: 2025-09-01 11:00
venue: TOLANSKY 125 
link: https://teams.microsoft.com/l/meetup-join/19%3ameeting_NGRhZjQ1OGMtNzA4Mi00MWMxLWE1ODEtNzJjNmEwZjBkZTg1%40thread.v2/0?context=%7b%22Tid%22%3a%222efd699a-1922-4e69-b601-108008d28a2e%22%2c%22Oid%22%3a%229c6a898b-86f6-4344-8f36-27129ffe3748%22%7d 
recording: https://rhul-my.sharepoint.com/:v:/g/personal/anand_subramoney_rhul_ac_uk/EXOwYdqbzl5NmCZF8vWIJQIB1el7L57aUsj4Kv6UtyOtjA?e=b3NkYY&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D 
bio: >
  Rafael Izbicki is a faculty member in the Department of Statistics at the Federal University of São Carlos, Brazil, and has been a CNPq research fellow since 2017. He earned his BSc (2009) and MSc (2010) in from the University of São Paulo and his PhD (2014) from Carnegie Mellon University. Rafael leads the Statistical Machine Learning Lab, with over 80 peer-reviewed publications and is the author of the book Machine Learning Beyond Point Predictions: Uncertainty Quantification. His research covers fundamental aspects of statistics and machine learning, with applications in biology, medicine, epidemiology, and cosmology.

abstract: >
  Building confidence sets that reach their nominal level is especially hard in complex models and when the observed sample size is small, as in many likelihood-free inference tasks. We introduce a simulation-based calibration scheme that constructs local confidence sets using only samples drawn from the data-generating model. Adapting insights from conformal prediction to parameter inference, the method forms data-adaptive partitions that deliver finite-sample local coverage and achieve conditional coverage as the number of simulations grows. It handles nuisance parameters with no substantial computational cost and computes uncertainty estimates that flag when more simulations are needed. Experiments on models with and without tractable likelihoods show that the resulting sets are better calibrated than existing alternatives. The procedure provides a practical tool for reliable inference in modern statistical applications.

---
